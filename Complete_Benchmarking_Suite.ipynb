{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxj_rNjiFYsR"
      },
      "outputs": [],
      "source": [
        "# benchmark_suite.py\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from Bio.Align.Applications import ClustalOmegaCommandline, MuscleCommandline\n",
        "import subprocess\n",
        "from youshan import YouShanAligner, youshan_progressive_msa\n",
        "\n",
        "class MSABenchmarkSuite:\n",
        "    def __init__(self, datasets_dir=\"benchmark_datasets\"):\n",
        "        self.datasets_dir = datasets_dir\n",
        "        self.results = []\n",
        "\n",
        "    def run_benchmark(self, dataset_name, tool_name, input_fasta):\n",
        "        \"\"\"Run MSA tool on dataset and measure performance\"\"\"\n",
        "        output_fasta = f\"results/{dataset_name}_{tool_name}_aligned.fasta\"\n",
        "        metrics = {'dataset': dataset_name, 'tool': tool_name}\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "\n",
        "            if tool_name == \"youshan\":\n",
        "                model = YouShanAligner()\n",
        "                sequences = [str(record.seq) for record in SeqIO.parse(input_fasta, \"fasta\")]\n",
        "                aligned_file, _, _ = youshan_progressive_msa(sequences, model)\n",
        "\n",
        "            elif tool_name == \"clustal\":\n",
        "                clustal_cmd = ClustalOmegaCommandline(\n",
        "                    infile=input_fasta,\n",
        "                    outfile=output_fasta,\n",
        "                    verbose=True,\n",
        "                    auto=True\n",
        "                )\n",
        "                subprocess.run(str(clustal_cmd), shell=True, check=True)\n",
        "\n",
        "            elif tool_name == \"muscle\":\n",
        "                muscle_cmd = MuscleCommandline(\n",
        "                    input=input_fasta,\n",
        "                    out=output_fasta\n",
        "                )\n",
        "                subprocess.run(str(muscle_cmd), shell=True, check=True)\n",
        "\n",
        "            execution_time = time.time() - start_time\n",
        "            metrics['execution_time'] = execution_time\n",
        "\n",
        "            # Calculate alignment metrics\n",
        "            alignment_metrics = self.calculate_alignment_metrics(output_fasta)\n",
        "            metrics.update(alignment_metrics)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error running {tool_name} on {dataset_name}: {e}\")\n",
        "            metrics['error'] = str(e)\n",
        "\n",
        "        self.results.append(metrics)\n",
        "        return metrics\n",
        "\n",
        "    def calculate_alignment_metrics(self, aligned_fasta):\n",
        "        \"\"\"Calculate various alignment quality metrics\"\"\"\n",
        "        sequences = list(SeqIO.parse(aligned_fasta, \"fasta\"))\n",
        "\n",
        "        if not sequences:\n",
        "            return {'error': 'No sequences in alignment'}\n",
        "\n",
        "        # Basic metrics\n",
        "        aligned_lengths = [len(seq.seq) for seq in sequences]\n",
        "        avg_length = sum(aligned_lengths) / len(aligned_lengths)\n",
        "        length_variance = sum((x - avg_length) ** 2 for x in aligned_lengths) / len(aligned_lengths)\n",
        "\n",
        "        # Conservation score\n",
        "        max_len = max(aligned_lengths)\n",
        "        conservation_scores = []\n",
        "\n",
        "        for i in range(max_len):\n",
        "            column_chars = []\n",
        "            for seq in sequences:\n",
        "                if i < len(seq.seq):\n",
        "                    column_chars.append(seq.seq[i])\n",
        "            if column_chars:\n",
        "                most_common = max(set(column_chars), key=column_chars.count)\n",
        "                conservation = column_chars.count(most_common) / len(column_chars)\n",
        "                conservation_scores.append(conservation)\n",
        "\n",
        "        avg_conservation = sum(conservation_scores) / len(conservation_scores) if conservation_scores else 0\n",
        "\n",
        "        return {\n",
        "            'num_sequences': len(sequences),\n",
        "            'alignment_length': max_len,\n",
        "            'length_variance': length_variance,\n",
        "            'avg_conservation': avg_conservation,\n",
        "            'total_gaps': sum(seq.seq.count('-') for seq in sequences)\n",
        "        }\n",
        "\n",
        "    def run_complete_benchmark(self):\n",
        "        \"\"\"Run complete benchmark suite on all datasets\"\"\"\n",
        "        tools = ['youshan', 'clustal', 'muscle']\n",
        "        datasets = [f for f in os.listdir(self.datasets_dir) if f.endswith('.fasta')]\n",
        "\n",
        "        os.makedirs('results', exist_ok=True)\n",
        "\n",
        "        for dataset_file in datasets:\n",
        "            dataset_name = dataset_file.replace('.fasta', '')\n",
        "            input_path = os.path.join(self.datasets_dir, dataset_file)\n",
        "\n",
        "            print(f\"\\nBenchmarking {dataset_name}...\")\n",
        "            for tool in tools:\n",
        "                print(f\"  Running {tool}...\")\n",
        "                self.run_benchmark(dataset_name, tool, input_path)\n",
        "\n",
        "        # Save results\n",
        "        results_df = pd.DataFrame(self.results)\n",
        "        results_df.to_csv('benchmark_results.csv', index=False)\n",
        "\n",
        "        # Generate summary report\n",
        "        self.generate_report(results_df)\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def generate_report(self, results_df):\n",
        "        \"\"\"Generate comprehensive benchmark report\"\"\"\n",
        "        report = \"\"\"\n",
        "# You-Shan MSA Benchmark Report\n",
        "\n",
        "## Summary\n",
        "This report presents the benchmarking results of You-Shan against traditional MSA tools.\n",
        "\n",
        "## Methods\n",
        "- **You-Shan**: Fine-tuned transformer-based MSA\n",
        "- **ClustalÎ©**: Progressive alignment with guide trees\n",
        "- **MUSCLE**: Multiple sequence comparison by log-expectation\n",
        "\n",
        "## Results\n",
        "\"\"\"\n",
        "\n",
        "        # Add results summary\n",
        "        summary = results_df.groupby('tool').agg({\n",
        "            'execution_time': 'mean',\n",
        "            'avg_conservation': 'mean',\n",
        "            'length_variance': 'mean'\n",
        "        }).round(3)\n",
        "\n",
        "        report += \"\\n### Performance Summary\\n\"\n",
        "        report += summary.to_markdown()\n",
        "\n",
        "        # Save report\n",
        "        with open('benchmark_report.md', 'w') as f:\n",
        "            f.write(report)\n",
        "\n",
        "        print(\"Benchmark report generated: benchmark_report.md\")\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    benchmark = MSABenchmarkSuite()\n",
        "    results = benchmark.run_complete_benchmark()"
      ]
    }
  ]
}